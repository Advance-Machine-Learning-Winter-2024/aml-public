{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    "scibert='allenai/scibert_scivocab_uncased'\n",
    "specter='allenai/specter'\n",
    "sbert='Muennighoff/SBERT-base-nli-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(sbert)\n",
    "tokens=tokenizer(\"Hello World\", padding=True, truncation=True, return_tensors='pt',max_length=512)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(tokens['input_ids'].shape[1]):\n",
    "    print(i,tokens['input_ids'][0,i],tokenizer.decode(tokens['input_ids'][0,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode text\n",
    "def encode(texts,modelname,strat='pool'):\n",
    "    #Mean Pooling - Take average of all tokens\n",
    "    def mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output.last_hidden_state\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "    model = AutoModel.from_pretrained(modelname)\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(texts, padding=True, truncation=True, \n",
    "                              return_tensors='pt',max_length=512)\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input, return_dict=True)\n",
    "    # Perform pooling\n",
    "    if strat=='pool':\n",
    "        embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "    elif strat=='cls':\n",
    "        embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    return embeddings.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2, use_torch=True):\n",
    "    if use_torch:\n",
    "        dot_product = torch.dot(vector1, vector2)\n",
    "        norm_vector1 = torch.norm(vector1)\n",
    "        norm_vector2 = torch.norm(vector2)\n",
    "        similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    else:\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        norm_vector1 = np.linalg.norm(vector1)\n",
    "        norm_vector2 = np.linalg.norm(vector2)\n",
    "        similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "senL=['Hello World','Great day today','Hi there and greetings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode(senL[0],specter,strat='cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GEMINI=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genL=senL\n",
    "strat='pool'\n",
    "bert_embeddingsL=[]\n",
    "sbert_embeddingsL=[]\n",
    "scibert_embeddingsL=[]\n",
    "specter_embeddingsL=[]\n",
    "if USE_GEMINI:gemini_embeddingsL=[]\n",
    "for s in genL:\n",
    "    bert_embeddingsL.append(encode(s,bert,strat))\n",
    "    sbert_embeddingsL.append(encode(s,sbert,strat))\n",
    "    scibert_embeddingsL.append(encode(s,scibert,strat))\n",
    "    specter_embeddingsL.append(encode(s,specter,strat))\n",
    "    if USE_GEMINI:gemini_embeddingsL.append(encode_gemini(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_distances=np.zeros((3,3))\n",
    "sbert_distances=np.zeros((3,3))\n",
    "scibert_distances=np.zeros((3,3))\n",
    "specter_distances=np.zeros((3,3))\n",
    "if USE_GEMINI:gemini_distances=np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i<j:break\n",
    "        bert_distances[i,j]=cosine_similarity(bert_embeddingsL[i],bert_embeddingsL[j])\n",
    "        sbert_distances[i,j]=cosine_similarity(sbert_embeddingsL[i],sbert_embeddingsL[j])\n",
    "        scibert_distances[i,j]=cosine_similarity(scibert_embeddingsL[i],scibert_embeddingsL[j])\n",
    "        specter_distances[i,j]=cosine_similarity(specter_embeddingsL[i],specter_embeddingsL[j])\n",
    "        if USE_GEMINI:gemini_distances[i,j]=cosine_similarity(gemini_embeddingsL[i],gemini_embeddingsL[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scibert_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specter_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GEMINI:\n",
    "    print(gemini_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiL=['Deep Learning','Artificial Intelligence','Programming Languages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genL=aiL\n",
    "strat='pool'\n",
    "bert_embeddingsL=[]\n",
    "sbert_embeddingsL=[]\n",
    "scibert_embeddingsL=[]\n",
    "specter_embeddingsL=[]\n",
    "if USE_GEMINI:gemini_embeddingsL=[]\n",
    "for s in genL:\n",
    "    bert_embeddingsL.append(encode(s,bert,strat))\n",
    "    sbert_embeddingsL.append(encode(s,sbert,strat))\n",
    "    scibert_embeddingsL.append(encode(s,scibert,strat))\n",
    "    specter_embeddingsL.append(encode(s,specter,strat))\n",
    "    if USE_GEMINI:gemini_embeddingsL.append(encode_gemini(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_distances=np.zeros((3,3))\n",
    "sbert_distances=np.zeros((3,3))\n",
    "scibert_distances=np.zeros((3,3))\n",
    "specter_distances=np.zeros((3,3))\n",
    "if USE_GEMINI:gemini_distances=np.zeros((3,3))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        if i<j:break\n",
    "        bert_distances[i,j]=cosine_similarity(bert_embeddingsL[i],bert_embeddingsL[j])\n",
    "        sbert_distances[i,j]=cosine_similarity(sbert_embeddingsL[i],sbert_embeddingsL[j])\n",
    "        scibert_distances[i,j]=cosine_similarity(scibert_embeddingsL[i],scibert_embeddingsL[j])\n",
    "        specter_distances[i,j]=cosine_similarity(specter_embeddingsL[i],specter_embeddingsL[j])\n",
    "        if USE_GEMINI:gemini_distances[i,j]=cosine_similarity(gemini_embeddingsL[i],gemini_embeddingsL[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scibert_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specter_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GEMINI:\n",
    "    print(gemini_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anvil.server\n",
    "import import_ipynb\n",
    "from mykeys import GEMENIKEY,SERVERKEY\n",
    "anvil.server.connect(SERVERKEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_gemini(text):\n",
    "    return torch.tensor(anvil.server.call('encode_gemini',text,GEMENIKEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_gemini('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import google.generativeai as genai\n",
    "# from mykeys import GEMENIKEY\n",
    "# genai.configure(api_key=GEMENIKEY)\n",
    "# def encode_gemini(text):\n",
    "#     result = genai.embed_content(\n",
    "#     model=\"models/embedding-001\",\n",
    "#     content=text,\n",
    "#     task_type=\"retrieval_document\",\n",
    "#     title=\"Embedding of single string\")\n",
    "#     return result['embedding']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310exp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
